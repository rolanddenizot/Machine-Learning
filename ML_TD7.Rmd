---
title: "Denizot_Roland_Jeanneney_Mewen_PW7"
subtitle: "Clustering"
author: "Roland Denizot and MÃ©wen Jeanneney"
date: "`r format(Sys.time())`"
output:
  html_document:
    toc: true
    toc_depth: 2
    theme: flatly
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# K-Means Clustering

## Question 1
```{r Question 1}
ligue1 = read.csv('http://mghassany.com/MLcourse/datasets/ligue1_17_18.csv', row.names=1, sep=';')
```

## Question 2
```{r Question 2}
knitr::kable(ligue1[1:2,])
print(dim(ligue1)[2])
```

# PointsCards

## Question 3
```{r Question 3}
pointsCards = ligue1[, c('Points', 'yellow.cards')]
```

## Question 4
```{r Question 4}
km = kmeans(pointsCards, centers = 2, iter.max = 20)
```

## Question 5
```{r Question 5}
print(km)
str(km)
```

## Question 6
```{r Question 6}
print(km$centers)
```

## Question 7
```{r Question 7}
plot(pointsCards, km$cluster, col = km$cluster, pch = 19, cex = 2)
```

## Question 8
```{r Question 8}
plot(pointsCards, col = km$cluster, pch = 19, cex = 2)
points(km$centers, col = 1:3, pch = 3, cex = 3, lwd = 3)
text(pointsCards, row.names(pointsCards), cex = 0.8, col = km$cluster, pos = 3)
```

## Question 9
```{r Question 9}
km3 = kmeans(pointsCards, centers = 3, iter.max = 20)
plot(pointsCards, col = km3$cluster, pch = 19, cex = 2)
points(km3$centers, col = 1:3, pch = 3, cex = 3, lwd = 3)
text(pointsCards, row.names(pointsCards), cex = 0.8, col = km3$cluster, pos = 3)
```
```{r Question 9bis}
km4 = kmeans(pointsCards, centers = 4, iter.max = 20)
plot(pointsCards, col = km4$cluster, pch = 19, cex = 2)
points(km4$centers, col = 1:4, pch = 3, cex = 3, lwd = 3)
text(pointsCards, row.names(pointsCards), cex = 0.8, col = km4$cluster, pos = 3)
```

## Question 10
```{r Question 10}
mydata <- pointsCards
wss <- (nrow(mydata)-1)*sum(apply(mydata,2,var))

for (i in 2:15) {
  wss[i] <- sum(kmeans(mydata,centers=i)$withinss)}

plot(1:15, wss, type='b', xlab='Number of Clusters',ylab='Within groups sum of squares')

```

## Question 11
```{r Question 11}
mydata <- pointsCards
wss <- (nrow(mydata)-1)*sum(apply(mydata,2,var))

for (i in 2:15) {
  wss[i] <- sum(kmeans(mydata, centers=i)$betweenss/kmeans(mydata, centers=i)$totss)}

plot(2:15, wss[2:15], type='b', xlab='Number of Clusters',ylab='Between_SS/Total_SS')
```
Only 3 or 4 clusters is enough to have a good result, more is considered as overfittings

# Ligue 1

## Question 12
```{r Question 12}
ligue1_scaled = data.frame(scale(ligue1))
ligue1_scaled
```

## Question 13
```{r Question 13}
set.seed(100)
km.ligue1 = kmeans(ligue1, centers = 3, iter.max = 20)
km.ligue1.scaled = kmeans(ligue1_scaled, centers = 3, iter.max = 20)
```

## Question 14
```{r Question 14}
table(km.ligue1$cluster)
table(km.ligue1.scaled$cluster)
```
We don't obtain the same results on scaled and unscaled data

# PCA

## Question 15
```{r Question 15}
library("FactoMineR")
#pcaligue1 = PCA(ligue1, scale.unit = TRUE)
pcaligue1 = princomp(ligue1, cor = TRUE)
```
We don't need apply PCA on the scaled data set car cor = TRUE is doing the same thing

## Question 16
```{r Question 16}
library("factoextra")
fviz_pca_biplot(pcaligue1)
```
The first variable explains 63.5% of the graph and the second variable explains 10.1% of the graph

## Question 17
```{r Question 17}
fviz_cluster(km.ligue1, data = ligue1, # km.ligue1 is where you stored your kmeans results
              palette = c('red', 'blue', 'green'), # 3 colors since 3 clusters
              ggtheme = theme_minimal(),
              main = "Clustering Plot")
```

## Question 18
```{r Question 18}
set.seed(101)
km3 = kmeans(pcaligue1$scores[,1:2], centers = 3, iter.max = 20)
fviz_cluster(km3, data = pcaligue1$scores[, 1:2], 
              palette = c('red', 'blue', 'green'), 
              ggtheme = theme_minimal(),
              main = 'Clustering Plot on the 2 First PCs')
```
The clusters are not a the same place but they contain almost the same Clubs in each ones


# Implementing k-means

## Question 19
```{r Question 19}
X1 = c(1,1,0,5,6,4)
X2 = c(4,3,4,1,2,0)
df = data.frame(X1, X2)
print (df)
plot(df[,1],df[,2], pch=19, cex=2)
```

## Question 20
```{r Question 20}
labels = sample(2, nrow(df), replace = TRUE)
labels
```

## Question 21
```{r Question 21}
centroid1 = c(mean(df[labels==1, 1]), mean(df[labels==1, 2]))
centroid2 = c(mean(df[labels==2, 1]), mean(df[labels==2, 2]))
print(centroid1)
print(centroid2)

plot(df[,1], df[,2], pch=19, cex=2)
points(centroid1[1], centroid1[2], pch = 3, cex = 3, lwd = 3)
points(centroid2[1], centroid2[2], pch = 3, cex = 3, lwd = 3)
```

## Question 22
```{r Question 22}
euclidianDistance = function(point1,point2){
  return(sqrt((point1[1]-point2[1])^2 + (point1[2]-point2[2])^2))}
```

## Question 23
```{r Question 23}
labels = rep(0,nrow(df))
for (i in 1:nrow(df)){
  if (euclidianDistance(df[i,], centroid1) < euclidianDistance(df[i,], centroid2))
    labels[i] = 1
  else
    labels[i] = 2
}
labels
```

## Question 24
```{r  Question 24}

```

## Question 25
```{r Question 25}
plot(df[,1],df[,2], pch = 19, cex=2, col = labels+1)
points(centroid1[1], centroid1[2], pch = 3, cex = 3, lwd = 3, col = 2)
points(centroid2[1], centroid2[2], pch = 3, cex = 3, lwd = 3, col = 3)
```

# Hierarchical clustering

## Question 1
```{r Question 101}
dataset <- read.csv('iris.csv')
dataset
```

## Question 2
```{r Question 102}
set.seed(102)
dataSample = dataset[sample(1:150,40),]
dataSample
```

## Question 3
```{r Question 103}
D = dist(dataSample[,1:4], method = 'euclidean')
```

## Question 4
```{r Question 104}
dendro.avg = hclust(D, method='average')
```

## Question 5
```{r Question 105}
plot(dendro.avg)
```

## Question 6
```{r Question 106}
plot(dendro.avg, hang=-1, label=dataSample$class)
```

## Question 7
```{r Question 107}
groups.avg = cutree(dendro.avg, k = 3)
```

## Question 8
```{r Question 108}
plot(dendro.avg, hang=-1, label=dataSample$class)
rect.hclust(dendro.avg, 3, border = c('red','blue','green'))
```

## Question 9
```{r Question 109}
table(groups.avg, dataSample$class)
```
It seems to have only 2 wrong results, so it is a 95% Accuracy


## Question 10
```{r Question 110}
D = dist(dataset[,1:4], method = 'euclidean')
dendro.avg = hclust(D, method='average')
plot(dendro.avg, hang=-1, label=dataset$class)
groups.avg = cutree(dendro.avg, k = 3)
plot(dendro.avg, hang=-1, label=dataset$class)
rect.hclust(dendro.avg, 3, border = c('red','blue','green'))
table(groups.avg, dataset$class)
```
90,6% Accuracy
```{r Question 1101}
dendro.avg = hclust(D, method='complete')
plot(dendro.avg, hang=-1, label=dataset$class)
groups.avg = cutree(dendro.avg, k = 3)
plot(dendro.avg, hang=-1, label=dataset$class)
rect.hclust(dendro.avg, 3, border = c('red','blue','green'))
table(groups.avg, dataset$class)
```
84% Accuracy
```{r Question 1102}
dendro.avg = hclust(D, method='single')
plot(dendro.avg, hang=-1, label=dataset$class)
groups.avg = cutree(dendro.avg, k = 3)
plot(dendro.avg, hang=-1, label=dataset$class)
rect.hclust(dendro.avg, 3, border = c('red','blue','green'))
table(groups.avg, dataset$class)
```
68% Accuracy

The best method seems to be the Hierarchical Clustering Method "Average"